
// // Fungsi untuk upload data ditiap-tiap data CSV (baik master maupun data yang akan diproses)
// // Map of collection names to corresponding structs for header validation
// var collectionStructs = map[string]interface{}{
// 	"datakof":      models.Datakof{},
// 	"datakif":      models.Datakif{},
// 	"datasof":      models.Datasof{},
// 	"datasif":      models.Datasif{},
// 	"datapof":      models.Datapof{},
// 	"datafro":      models.Datafro{},
// 	"datafrd":      models.Datafrd{},
// 	"mastermn_1":   models.Mastermn_1{},
// 	"masteric_2":   models.Masteric_2{},
// 	"masterls_3":   models.Masterls_3{},
// 	"mastertbs_4":  models.Mastertbs_4{},
// 	"mastertbs_41": models.Mastertbs_41{},
// 	"masterbc_5":   models.Masterbc_5{},
// 	"masterrg_6":   models.Masterrg_6{},
// }

// // Map of collections requiring data removal before upload
// var collectionsToClear = map[string]bool{
// 	"datakof":      true, // Use DeleteMany for this collection
// 	"datakif":      true, // Use DeleteMany for this collection
// 	"datasof":      true, // Use DeleteMany for this collection
// 	"datasif":      true,
// 	"datapof":      true,
// 	"datafro":      true,
// 	"datafrd":      true,
// 	"mastermn_1":   true,  // Use DeleteMany for this collection
// 	"masteric_2":   false, // Skip data removal
// 	"masterls_3":   false, // Skip data removal
// 	"mastertbs_4":  false, // Skip data removal
// 	"mastertbs_41": false,
// 	"masterbc_5":   false, // Skip data removal
// 	"masterrg_6":   false, // Skip data removal
// }

// // Utility function to trim spaces and replace special characters with a space
// func TrimSpacesAndRemoveChar160(input string) string {
// 	// Step 1: Trim leading and trailing spaces
// 	trimmed := strings.TrimSpace(input)

// 	// Step 2: Replace special characters and remove unrecognized characters
// 	return strings.Map(func(r rune) rune {
// 		if r == 160 || !unicode.IsPrint(r) { // Replace non-breaking space and non-printable characters
// 			return ' ' // Replace with a regular space
// 		}
// 		return r // Keep printable characters as-is
// 	}, trimmed)
// }
// func respondWithJSONError(w http.ResponseWriter, statusCode int, message string) {
// 	w.Header().Set("Content-Type", "application/json")
// 	w.WriteHeader(statusCode)
// 	json.NewEncoder(w).Encode(map[string]string{"error": message})
// }

// // Get the list of expected headers from struct tags
// func getStructHeaders(structType interface{}) []string {
// 	v := reflect.TypeOf(structType)
// 	if v.Kind() == reflect.Ptr {
// 		v = v.Elem()
// 	}

// 	var headers []string
// 	for i := 0; i < v.NumField(); i++ {
// 		field := v.Field(i)
// 		tag := field.Tag.Get("bson")
// 		if tag != "" && tag != "-" {
// 			headers = append(headers, tag)
// 		}
// 	}
// 	return headers
// }

// // Validate CSV headers against the expected struct headers
// func validateHeaders(csvHeaders, expectedHeaders []string) error {
// 	if len(csvHeaders) != len(expectedHeaders) {
// 		return fmt.Errorf("header count mismatch: expected %d, got %d", len(expectedHeaders), len(csvHeaders))
// 	}
// 	for i, csvHeader := range csvHeaders {
// 		if csvHeader != expectedHeaders[i] {
// 			return fmt.Errorf("header mismatch at column %d: expected '%s', got '%s'", i+1, expectedHeaders[i], csvHeader)
// 		}
// 	}
// 	return nil
// }

// // GenerateUniqueKey creates a unique key by hashing the concatenated record fields
// func GenerateUniqueKey(record map[string]interface{}) string {
// 	hash := sha256.New()
// 	for key, value := range record {
// 		hash.Write([]byte(fmt.Sprintf("%s:%v;", key, value)))
// 	}
// 	return hex.EncodeToString(hash.Sum(nil))
// }

// // UploadData handles CSV uploads to specified MongoDB collections
// func UploadData(w http.ResponseWriter, r *http.Request) {
// 	collectionName := r.URL.Query().Get("collection")
// 	if collectionName == "" {
// 		respondWithJSONError(w, http.StatusBadRequest, "Collection name is required")
// 		return
// 	}

// 	// Validate the collection name
// 	structType, exists := collectionStructs[collectionName]
// 	if !exists {
// 		respondWithJSONError(w, http.StatusBadRequest, fmt.Sprintf("Unknown collection: %s", collectionName))
// 		return
// 	}

// 	// Parse the CSV file
// 	file, _, err := r.FormFile("file")
// 	if err != nil {
// 		respondWithJSONError(w, http.StatusBadRequest, "Failed to read CSV file: "+err.Error())
// 		return
// 	}
// 	defer file.Close()

// 	reader := csv.NewReader(file)

// 	// Read CSV headers
// 	csvHeaders, err := reader.Read()
// 	if err != nil {
// 		respondWithJSONError(w, http.StatusBadRequest, "Failed to read CSV headers: "+err.Error())
// 		return
// 	}

// 	// Get expected headers from the struct
// 	expectedHeaders := getStructHeaders(structType)
// 	if err := validateHeaders(csvHeaders, expectedHeaders); err != nil {
// 		respondWithJSONError(w, http.StatusBadRequest, "Header validation failed: "+err.Error())
// 		return
// 	}

// 	// Read CSV records
// 	records, err := reader.ReadAll()
// 	if err != nil {
// 		respondWithJSONError(w, http.StatusBadRequest, "Failed to read CSV data: "+err.Error())
// 		return
// 	}

// 	// Prepare data for MongoDB with duplicate removal
// 	var data []interface{}
// 	seenRecords := make(map[string]struct{}) // Map to track unique records
// 	for _, record := range records {
// 		row := make(map[string]interface{})
// 		for i, value := range record {
// 			// Clean up each field using the utility function
// 			row[csvHeaders[i]] = TrimSpacesAndRemoveChar160(value)
// 		}

// 		// Generate a unique key for the record
// 		uniqueKey := GenerateUniqueKey(row)
// 		if _, exists := seenRecords[uniqueKey]; exists {
// 			continue // Skip duplicate record
// 		}
// 		seenRecords[uniqueKey] = struct{}{} // Mark record as seen
// 		data = append(data, row)
// 	}

// 	// Get the MongoDB collection
// 	collection := db.GetCollection(collectionName)

// 	// Remove existing data only for collections that require it
// 	if shouldClear, ok := collectionsToClear[collectionName]; ok && shouldClear {
// 		if _, err := collection.DeleteMany(context.Background(), bson.D{}); err != nil {
// 			log.Printf("Failed to clear data in collection %s: %v", collectionName, err)
// 			respondWithJSONError(w, http.StatusInternalServerError, "Failed to clear data in collection")
// 			return
// 		}
// 		log.Printf("Data in collection %s cleared successfully", collectionName)
// 	}

// 	// Different handling for collections `datakof` and `mastermn_1`
// 	if collectionName == "datakof" || collectionName == "datakif" || collectionName == "mastermn_1" {
// 		// Use InsertMany for these collections
// 		const batchSize = 1000
// 		var totalInserted int
// 		for i := 0; i < len(data); i += batchSize {
// 			end := i + batchSize
// 			if end > len(data) {
// 				end = len(data)
// 			}

// 			batch := data[i:end]
// 			_, err := collection.InsertMany(context.Background(), batch)
// 			if err != nil {
// 				log.Printf("Failed to insert batch into collection %s: %v", collectionName, err)
// 				respondWithJSONError(w, http.StatusInternalServerError, "Failed to insert data into database")
// 				insertUploadLog(collectionName, totalInserted, "Failed", err.Error())
// 				return
// 			}
// 			totalInserted += len(batch)
// 			log.Printf("Inserted batch of %d records into collection %s", len(batch), collectionName)
// 		}
// 		insertUploadLog(collectionName, totalInserted, "Success", "")
// 	} else if collectionName == "masterls_3" {
// 		// Special handling for masterls_3: Update all fields and Last Status for duplicates based on "STT No"
// 		const batchSize = 1000
// 		var totalInserted int
// 		for i := 0; i < len(data); i += batchSize {
// 			end := i + batchSize
// 			if end > len(data) {
// 				end = len(data)
// 			}

// 			batch := data[i:end]

// 			// Prepare bulk operations
// 			var bulkOps []mongo.WriteModel
// 			for _, record := range batch {
// 				doc := record.(map[string]interface{})

// 				// Use "STT No" as the unique key for filtering
// 				sttNo := doc["STT No"]

// 				// Update entire document and specifically set "Last Status"
// 				filter := bson.M{"STT No": sttNo}
// 				update := bson.M{"$set": doc} // Update all fields, including "Last Status"

// 				// Append the update operation
// 				bulkOps = append(bulkOps, mongo.NewUpdateOneModel().SetFilter(filter).SetUpdate(update).SetUpsert(true))
// 			}

// 			// Perform BulkWrite for masterls_3
// 			bulkOpts := options.BulkWrite().SetOrdered(false)
// 			res, err := collection.BulkWrite(context.Background(), bulkOps, bulkOpts)
// 			if err != nil {
// 				log.Printf("Failed to perform bulk write for collection %s: %v", collectionName, err)
// 				respondWithJSONError(w, http.StatusInternalServerError, "Failed to insert or update data in database")
// 				insertUploadLog(collectionName, totalInserted, "Failed", err.Error())
// 				return
// 			}

// 			totalInserted += len(batch)
// 			log.Printf("Bulk write result - Matched: %d, Modified: %d, Upserted: %d in collection %s", res.MatchedCount, res.ModifiedCount, res.UpsertedCount, collectionName)
// 		}

// 		insertUploadLog(collectionName, totalInserted, "Success", "")
// 	} else {
// 		// Use BulkWrite for other collections
// 		const batchSize = 1000
// 		var totalInserted int
// 		for i := 0; i < len(data); i += batchSize {
// 			end := i + batchSize
// 			if end > len(data) {
// 				end = len(data)
// 			}

// 			batch := data[i:end]

// 			// Prepare bulk operations
// 			var bulkOps []mongo.WriteModel
// 			for _, record := range batch {
// 				doc := record.(map[string]interface{})

// 				// Determine the unique key field for the collection
// 				var uniqueKeyField string
// 				switch collectionName {
// 				case "masteric_2":
// 					uniqueKeyField = "STT No"
// 				case "mastertbs_4":
// 					uniqueKeyField = "Kode"
// 				case "masterbc_5":
// 					uniqueKeyField = "Stt ID"
// 				case "masterrg_6":
// 					uniqueKeyField = "Productroute"
// 				default:
// 					uniqueKeyField = "_id" // Replace with an appropriate default field
// 				}

// 				// Use the unique key field for filtering
// 				filter := bson.M{uniqueKeyField: doc[uniqueKeyField]}
// 				update := bson.M{"$set": doc}

// 				// Append the update operation with upsert
// 				bulkOps = append(bulkOps, mongo.NewUpdateOneModel().SetFilter(filter).SetUpdate(update).SetUpsert(true))
// 			}

// 			// Perform bulk write
// 			bulkOpts := options.BulkWrite().SetOrdered(false)
// 			res, err := collection.BulkWrite(context.Background(), bulkOps, bulkOpts)
// 			if err != nil {
// 				log.Printf("Failed to perform bulk write for collection %s: %v", collectionName, err)
// 				respondWithJSONError(w, http.StatusInternalServerError, "Failed to insert or update data in database")
// 				insertUploadLog(collectionName, totalInserted, "Failed", err.Error())
// 				return
// 			}

// 			totalInserted += len(batch)
// 			log.Printf("Bulk write result - Matched: %d, Modified: %d, Upserted: %d in collection %s", res.MatchedCount, res.ModifiedCount, res.UpsertedCount, collectionName)
// 		}
// 		insertUploadLog(collectionName, totalInserted, "Success", "")
// 	}

// 	// Success response
// 	log.Printf("Successfully inserted %d records into collection %s", len(data), collectionName)
// 	w.Header().Set("Content-Type", "application/json")
// 	w.WriteHeader(http.StatusOK)
// 	json.NewEncoder(w).Encode(map[string]string{"message": "Data uploaded successfully"})
// }

// // Function to insert upload logs
// func insertUploadLog(collectionName string, recordCount int, status, errorMessage string) {
// 	logCollection := db.GetCollection("update_logs")
// 	logEntry := models.UploadLog{
// 		CollectionName: collectionName,
// 		RecordCount:    recordCount,
// 		UploadedAt:     time.Now(),
// 		Status:         status,
// 		ErrorMessage:   errorMessage,
// 	}
// 	if _, err := logCollection.InsertOne(context.Background(), logEntry); err != nil {
// 		log.Printf("Failed to insert upload log for collection %s: %v", collectionName, err)
// 	}
// }

// // Print ke CSV Header untuk panduan team LP
// func FetchHeadersAndExport(w http.ResponseWriter, r *http.Request) {
// 	collectionName := r.URL.Query().Get("collection")
// 	if collectionName == "" {
// 		http.Error(w, "Collection name is required", http.StatusBadRequest)
// 		return
// 	}

// 	// Validate the collection name and get the struct type
// 	structType, exists := collectionStructs[collectionName]
// 	if !exists {
// 		http.Error(w, fmt.Sprintf("Unknown collection: %s", collectionName), http.StatusBadRequest)
// 		return
// 	}

// 	// Get headers from the struct
// 	headers := getStructHeaders(structType)
// 	if len(headers) == 0 {
// 		http.Error(w, fmt.Sprintf("No headers found for collection: %s", collectionName), http.StatusInternalServerError)
// 		return
// 	}

// 	// Set response headers for CSV download
// 	w.Header().Set("Content-Type", "text/csv")
// 	w.Header().Set("Content-Disposition", fmt.Sprintf("attachment;filename=%s_headers.csv", collectionName))

// 	// Write headers to CSV
// 	csvWriter := csv.NewWriter(w)
// 	defer csvWriter.Flush()

// 	if err := csvWriter.Write(headers); err != nil {
// 		log.Printf("Failed to write headers to CSV: %v", err)
// 		http.Error(w, "Failed to write CSV file", http.StatusInternalServerError)
// 		return
// 	}

// 	log.Printf("Headers exported for collection %s: %v", collectionName, headers)
// }
// first code that work
